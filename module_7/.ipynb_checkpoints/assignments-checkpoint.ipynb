{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Working-with-Files\" data-toc-modified-id=\"Working-with-Files-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Working with Files</a></span></li><li><span><a href=\"#Plotting-Basics\" data-toc-modified-id=\"Plotting-Basics-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Plotting Basics</a></span></li><li><span><a href=\"#Summary-Stats\" data-toc-modified-id=\"Summary-Stats-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Summary Stats</a></span><ul class=\"toc-item\"><li><span><a href=\"#Question-1\" data-toc-modified-id=\"Question-1-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Question 1</a></span></li><li><span><a href=\"#Question-2\" data-toc-modified-id=\"Question-2-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Question 2</a></span></li><li><span><a href=\"#Question-3\" data-toc-modified-id=\"Question-3-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Question 3</a></span></li><li><span><a href=\"#Question-4\" data-toc-modified-id=\"Question-4-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Question 4</a></span></li><li><span><a href=\"#Question-5\" data-toc-modified-id=\"Question-5-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Question 5</a></span></li></ul></li><li><span><a href=\"#Basics-of-Probability\" data-toc-modified-id=\"Basics-of-Probability-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Basics of Probability</a></span><ul class=\"toc-item\"><li><span><a href=\"#Drill-1\" data-toc-modified-id=\"Drill-1-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Drill 1</a></span></li><li><span><a href=\"#Drill-2\" data-toc-modified-id=\"Drill-2-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Drill 2</a></span></li></ul></li><li><span><a href=\"#Evaluating-data-sources\" data-toc-modified-id=\"Evaluating-data-sources-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Evaluating data sources</a></span><ul class=\"toc-item\"><li><span><a href=\"#Question-1\" data-toc-modified-id=\"Question-1-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Question 1</a></span></li><li><span><a href=\"#Question-2\" data-toc-modified-id=\"Question-2-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Question 2</a></span></li><li><span><a href=\"#Question-3\" data-toc-modified-id=\"Question-3-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Question 3</a></span></li></ul></li><li><span><a href=\"#Sampling/Central-Theorem\" data-toc-modified-id=\"Sampling/Central-Theorem-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Sampling/Central Theorem</a></span><ul class=\"toc-item\"><li><span><a href=\"#Assignments\" data-toc-modified-id=\"Assignments-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Assignments</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import statistics\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#purchases = pd.read_csv('purchases.csv')\n",
    "#purchases.to_csv('test.csv')\n",
    "\n",
    "#j_purchases = pd.read_json('purchases.json')\n",
    "#j_purchases.to_json('json_purchases.json')\n",
    "lines = []\n",
    "with open('purchases.csv') as purchases:\n",
    "    purchases = purchases.readline()\n",
    "    for line in purchases:\n",
    "        lines.append(line)\n",
    "#\"\".join(lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Basics\n",
    "\n",
    "Let's go out into the world and generate some beautiful visuals. Pick a data source from this aggregation, load the data into a pandas data frame, and generate a series of visuals around that data using pyplot.\n",
    "\n",
    "Each visualization should be accompanied by 2-3 sentences describing what you think is revealed by this representation. Generate at least four different visuals, and be sure to use different types as well as the subplot functionality discussed above. And remember: clean and elegant visuals are key to telling a coherent story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import data\n",
    "df = pd.read_csv('planecrashinfo_20181121001952.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5783"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#explore\n",
    "\n",
    "#df.head()\n",
    "#df.describe()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#drop irrelevant colomns\n",
    "columns_drop = ['flight_no', 'ac_type', 'registration', \n",
    "                'cn_ln', 'aboard', 'ground', 'summary']\n",
    "\n",
    "df.drop(columns = columns_drop, inplace = True)\n",
    "\n",
    "#keep top 10 freq crash locations\n",
    "locations = []\n",
    "freq_loc = []\n",
    "cnt_loc = Counter(df['location'])\n",
    "common_loc = cnt_loc.most_common(10)\n",
    "for key, value in common_loc:\n",
    "    locations.append(key)\n",
    "    freq_loc.append(value)\n",
    "df = df[df['location'].isin(locations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"'September'\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-39b9ed24482f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m 'December': 12}\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'month_date_num'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmonths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'month_date_num'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'month_date_num'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-39b9ed24482f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     18\u001b[0m 'December': 12}\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'month_date_num'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmonths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'month_date_num'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'month_date_num'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"'September'\""
     ]
    }
   ],
   "source": [
    "df['total_fatalities'] = [i.split()[0]for i in df['fatalities']]\n",
    "\n",
    "#create column of only integer months\n",
    "df['month_date_num'] = [i.split(' ')[:1] for i in df['date']]\n",
    "pattern = r'[\\[\\]]'\n",
    "df['month_date_num'] = [re.sub(pattern, '', str(i)) for i in df['month_date_num']]\n",
    "\n",
    "\n",
    "months = {'Janauary': 1,\n",
    "'February': 2,\n",
    "'March': 3,\n",
    "'April': 4,\n",
    "'May':5,\n",
    "'June': 6,\n",
    "'July': 7,\n",
    "'August': 8,\n",
    "'September': 9,\n",
    "'October': 10,\n",
    "'November': 11,\n",
    "'December': 12}\n",
    "\n",
    "df['month_date_num'] = [months[i] for i in df['month_date_num']]\n",
    "df['month_date_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change time to integers between 0 - 23.59\n",
    "#effiecient way of converting strings to int?\n",
    "df.drop(df[df.time == '?'].index)\n",
    "df.time.value_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.total_fatalities.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore data\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "#\n",
    "plt.subplot(2,2,1)\n",
    "x = np.arange(0,20,2)\n",
    "plt.barh(locations, freq_loc)\n",
    "plt.xticks(x)\n",
    "plt.xlabel('Crash Frequency')\n",
    "plt.ylabel('Location')\n",
    "\n",
    "#year to fatalities\n",
    "plt.subplot(2,2,2)\n",
    "plt.scatter(df['time'], df['total_fatalities'])\n",
    "y = np.arange(1, 50, 5)\n",
    "plt.yticks(y)\n",
    "plt.xlabel('Time (Military)')\n",
    "# or ylabel can be just \"Fatalities\"?\n",
    "plt.ylabel('Fatalities per Crash')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-3 sentences description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kids = {'greg': 14,\n",
    "        'marcia': 12,\n",
    "        'peter': 11,\n",
    "        'jan': 10,\n",
    "        'bobby': 8,\n",
    "        'cindy': 6,\n",
    "        'oliver': 8}\n",
    "df = pd.DataFrame()\n",
    "df['names'] = kids.keys()\n",
    "df['age'] = kids.values()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 \n",
    "\n",
    "1. Greg was 14, Marcia was 12, Peter was 11, Jan was 10, Bobby was 8, and Cindy was 6 when they started playing the Brady kids on The Brady Bunch.  Cousin Oliver was 8 years old when he joined the show. What are the mean, median, and mode of the kids' ages when they first appeared on the show? What are the variance, standard deviation, and standard error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(df)\n",
    "median = np.median(df['age'])\n",
    "mode = statistics.mode(df['age'])\n",
    "var = np.var(df)\n",
    "std = np.std(df, ddof=1)\n",
    "stand_error = std / np.sqrt(len(df))\n",
    "stand_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "2. Using these estimates, if you had to choose only one estimate of central tendency and one estimate of variance to describe the data, which would you pick and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would choose the mean as the best measurement to describe the data. Since the dataset is very small it not beneficial to use the mode or median. As for variance, I would use the std since it will give insight on the age range of the data. Since the std is quite small, we can assume that the range of ages will be small too. The standard error wouldn't be beneficial since there is no uncertainty with the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "3. Next, Cindy has a birthday. Update your estimates- what changed, and what didn't?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#change cindy's age\n",
    "df.loc[5,'age'] = 9\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(df)\n",
    "median = np.median(df['age'])\n",
    "mode = statistics.mode(df['age'])\n",
    "var = np.var(df)\n",
    "std = np.std(df, ddof=1)\n",
    "stand_error = std / np.sqrt(len(df))\n",
    "stand_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean increased by around .5, std decreased by .5, standard error decreased by .1. The mode and median remained the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "4. Nobody likes Cousin Oliver. Maybe the network should have used an even younger actor. Replace Cousin Oliver with 1-year-old Jessica, then recalculate again.  Does this change your choice of central tendency or variance estimation methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[6, 'names'] = 'Jessica'\n",
    "df.loc[6, 'age'] = '1'\n",
    "#why does the describe attr change? \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "5. On the 50th anniversary of The Brady Bunch, four different magazines asked their readers whether they were fans of the show.  The answers were:\n",
    "    TV Guide            20% fans\n",
    "    Entertainment Weekly    23% fans\n",
    "    Pop Culture Today       17% fans\n",
    "    SciPhi Phanatic     5% fans\n",
    "\n",
    " Based on these numbers, what percentage of adult Americans would you estimate were Brady Bunch fans on the 50th anniversary of the show?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = np.mean([23, 20, 17])\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I actually didn't know what the Brady Bunch were so after I found out they were a sitcom, I realized that the SciPhi Phanatic community doesn't represent the population and is probably more biased. The other 3 probably represent the general population alot better. Knowing that, I took the average of the three and I would estimate that 20% of the adult Americans were fans of anniversary show. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drill 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the probability of flipping a balanced coin four times and getting each pattern: HTTH, HHHH and TTHH.\n",
    "If a list of people has 24 women and 21 men, then the probability of choosing a man from the list is 21/45. What is the probability of not choosing a man?\n",
    "The probability that Bernice will travel by plane sometime in the next year is 10%. The probability of a plane crash at any time is .005%. What is the probability that Bernice will be in a plane crash sometime in the next year?\n",
    "A data scientist wants to study the behavior of users on the company website. Each time a user clicks on a link on the website, there is a 5% chance that the user will be asked to complete a short survey about their behavior on the website. The data scientist uses the survey data to conclude that, on average, users spend 15 minutes surfing the company website before moving on to other things. What is wrong with this conclusion?\n",
    "When you're done, the solution for this drillset can be found here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "prob_h_t = .50\n",
    "htth = prob_h_t * prob_h_t * prob_h_t * prob_h_t\n",
    "answer = htth\n",
    "#since the prob are indep, HHHH and TTHH = 0.0625 too\n",
    "\n",
    "#2 \n",
    "total = 24 + 21\n",
    "answer = (45 - 21) / total\n",
    "\n",
    "#3 \n",
    "prob_travel = .10\n",
    "prob_crash = .005\n",
    "answer = prob_travel * prob_crash\n",
    "answer\n",
    "\n",
    "#4 \n",
    "prob_ask_survey = .05\n",
    "# biased towards users who use the website more and ultimately click more links.\n",
    "#since the likelihood of being asked increases everytime they click a link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drill 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drill set 2\n",
    "Now it's time to use Bayes' rule to compute some conditional probabilities. First look over the numbers and estimate each of the four probabilities, using your intuition. Then, calculate the probabilities using Bayes' rule. Keep track of your work in a Google document or markdown file that you can share with your mentor.\n",
    "\n",
    "A diagnostic test has a 98% probability of giving a positive result when applied to a person suffering from Thripshaw's Disease, and 10% probability of giving a (false) positive when applied to a non-sufferer. It is estimated that 0.5 % of the population are sufferers. Suppose that the test is now administered to a person whose disease status is unknown. Calculate the probability that the test will:\n",
    "\n",
    "Be positive \n",
    "\n",
    "Correctly diagnose a sufferer of Thripshaw's\n",
    "\n",
    "Correctly identify a non-sufferer of Thripshaw's\n",
    "\n",
    "Misclassify the person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes:\n",
    "P(A | B) = P(B | A) * P(A) / P(B)\n",
    " \n",
    "P(B | A) = prob of B (negative) given results A (false negative prob)\n",
    "\n",
    "P(A) = prob of A (prob of having disease (true positive prob)\n",
    "\n",
    "P(B) = prob B (prob of not having disease (true negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#typo in instructions .5 = .05\n",
    ".05 * .98 * .01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each of the scenarios, find possible shortcomings of the theoretical or actual data sources to answer the given question. What could be done to either adjust the analysis or reframe the question so that you can answer it accurately?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Data Source: Amsterdam availability data scraped from AirBnB on December 24th. Question: What are the popular neighborhoods in Amsterdam?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Amsterdam celebrates Christmas, the data will be skewed towards neighborhoods that are close to Christmas celebrations or events since 12/24 is Christmas eve. For a more accurate answer, the data source should be a on date that is far away from any culture or social events in Amsterdam. If the question was what are the most popular neighborhoods to celebrate christmas, then the data source would be reasonable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Source: Mental health services use on September 12, 2001 in San Francisco, CA and New York City, NY. Question: How do patterns of mental health service use vary between cities?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data source is the day after the tragic 9/11 attack. It was likely that the data is correlated to that event which may lead to more dramtic results that won't represent a general representation of patterns in the mental health services. The data will probably be probably be more skewed in New York since that was where the most damage and fatalities were. Although, one of the planes that was hijacked had bay area victims. \n",
    "\n",
    "There may be a chance that the data source may be accurate since the data was the day right after the event. I don't know how I would react if I were a victim's family member but my theory is that most people would not contact mental health services the day after. I would assume they would call maybe a couple to a few days after once they processed the situation and mourned with the rest of their family.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Data Source: Armenian Pub Survey. Question: What are the most common reasons Armenians visit local pubs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The population could be more specific/categorized. Instead of the data source generalizing Armenians, it should also include the person's age, gender, occupation, financial status, education, family, religion, etc. Providing these features will allow more insight into the reasons why they visit the local pubs. More specific features could be: frequency of going to the pubs, on average how long they stay at the pubs, do they go alone? (could help in finding if the reason is because of some emotional/addiction issue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling/Central Theorem\n",
    "\n",
    "## Assignments\n",
    "\n",
    "Now that you have some code to create your own populations, sample them, and compare the samples to the populations, it's time to experiment. Using your own Jupyter notebook, or a copy of the notebook above, reproduce the `pop1` and `pop2` populations and samples using numpy's binomial function. Specifically, create two binomially distributed populations with `n` equal to `10` and size equal to `10000`. The p-value of `pop1` should be `0.2` and the p-value of `pop2` should be `0.5`. Using a sample size of `100`, calculate the means and standard deviations of your samples.\n",
    "\n",
    "For each of the following tasks, first write what you expect will happen, then code the changes and observe what does happen.  Discuss the results with your mentor.\n",
    "\n",
    " 1. Increase the size of your samples from 100 to 1000, then calculate the means and standard deviations for your new samples and create histograms for each.  Repeat this again, decreasing the size of your samples to 20.  What values change, and what remain the same?\n",
    "\n",
    " 2. Change the probability value (`p` in the [NumPy documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.binomial.html)) for `pop1` to 0.3, then take new samples and compute the t-statistic and p-value.  Then change the probability value p for group 1 to 0.4, and do it again.  What changes, and why?\n",
    " \n",
    " 3. Change the distribution of your populations from binomial to a distribution of your choice.  Do the sample mean values still accurately represent the population values?\n",
    "\n",
    "When you've given it a try, you can find a sample solution [here](https://github.com/Thinkful-Ed/data-201-resources/blob/master/solutions/Prep%20course/3.3.7.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop1 = np.random.binomial(10, .2, 10000)\n",
    "pop2 = np.random.binomial(10, .5, 10000)\n",
    "\n",
    "samp1 = np.random.choice(pop1, 100)\n",
    "samp2 = np.random.choice(pop2, 100)\n",
    "m_1 = np.mean(samp1)\n",
    "m_2 = np.std(samp2)\n",
    "std_1 = np.mean(samp1)\n",
    "std_2 = np.std(samp2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
